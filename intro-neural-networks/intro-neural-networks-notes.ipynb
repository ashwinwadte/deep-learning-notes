{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concepts\n",
    "1. Intro to neural network and it's mathematics\n",
    "1. Intro to PyTorch\n",
    "1. Transfer learning\n",
    "1. CNN\n",
    "1. Style transfer\n",
    "1. RNN\n",
    "\n",
    "### data and exercises:\n",
    "1. MNIST\n",
    "1. CIFAR-100\n",
    "1. Cat-Dog\n",
    "1. Flower Images\n",
    "1. imdb movie database \n",
    "\n",
    "and exercises related to these\n",
    "\n",
    "#### Finally, we will do one project, where we will build, train, validate and test flower image classifier model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To display the quizzes, run following code cell once every new kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../helper')\n",
    "from quizzy import list_quiz, show_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron\n",
    "\n",
    "<img src=\"assets/linear_boundary.png\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3-D\n",
    "\n",
    "3-D visualization\n",
    "\n",
    "<img src=\"assets/three_dimension.png\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Higher dimensional\n",
    "<img src=\"assets/higher_dimension.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why neural network\n",
    "Perceptron looks like neurons in the brain.\n",
    "\n",
    "<img src=\"assets/neurons.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='percetron_trick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-Linear Regions\n",
    "\n",
    "What if we want to reject the student?\n",
    "\n",
    "<img src=\"assets/non_linear_reject_student.PNG\" width=900px>\n",
    "\n",
    "\n",
    "No matter what s/he gets in test, if grades are terrible, s/he will be rejected. So, data will look like this:\n",
    "\n",
    "\n",
    "<img src=\"assets/non_linear_data.PNG\" width=900px>\n",
    "\n",
    "\n",
    "How can this be separated?\n",
    "\n",
    "\n",
    "Circle\n",
    "<img src=\"assets/non_linear_circle.PNG\" width=900px>\n",
    "\n",
    "Two lines\n",
    "<img src=\"assets/non_linear_multi_lines.PNG\" width=900px>\n",
    "\n",
    "Curve\n",
    "<img src=\"assets/non_linear_curve.PNG\" width=900px>\n",
    "\n",
    "Let's go with curve...\n",
    "\n",
    "\n",
    "\n",
    "Unfortunately perceprton algorithm won't work this time. We need something more complex, we need to redefine perceptron algorithm for lines to generalize for other problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Funtions\n",
    "\n",
    "Error function tells us that how far we are from solution. It will tell us the distance and then we will look around and see which step takes us closer to the solution. Take that step and repeat.\n",
    "\n",
    "<img src=\"assets/mount_clouds.png\" width=900px>\n",
    "\n",
    "\n",
    "Here, key metric used to solve the problem is height. We will call the height - the error\n",
    "\n",
    "<img src=\"assets/mount_height.png\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to split data using error as no. of misclassified points:\n",
    "\n",
    "<img src=\"assets/split_data_error.PNG\" width=900px>\n",
    "\n",
    "When we try to descent, the error is still 2 in discrete\n",
    "<img src=\"assets/error_function_discrete_continuous.PNG\" width=900px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "<img src=\"assets/gd_error.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='conditions_for_gradient_descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete to Continuous Prediction\n",
    "\n",
    "<img src=\"assets/discrete_to_continuous_prediction.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/activation_function_step_to_sigmoid.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/sigmoid_prediction.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/logit_to_sigmoid.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/sigmoid_perceptron.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='perceptron_boundry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification and Softmax\n",
    "<img src=\"assets/softmax_1.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/softmax_2.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/softmax_3.PNG\" width=900px>\n",
    "\n",
    "### What could be the problems with this method?\n",
    "<img src=\"assets/softmax_4.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='convert_number_to_positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/softmax_5.PNG\" width=900px>\n",
    "\n",
    "## Softmax function and it's definition\n",
    "<img src=\"assets/softmax_6.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Coding Softmax\n",
    "And now, your time to shine! Let's code the formula for the Softmax function in Python.\n",
    "\n",
    "Refer exercise notebook for softmax coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "So far, all the algorithms are numerical. This means, we need to input numbers but the input data will not always look like numbers.\n",
    "\n",
    "<img src=\"assets/one_hot_encoding1.png\" width=900px>\n",
    "\n",
    "<img src=\"assets/one_hot_encoding2.png\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "Probability will be one of our best friends as we go through Deep Learning. In this lesson, we'll see how we can use probability to evaluate (and improve!) our models.\n",
    "\n",
    "\n",
    "Let's say I have two models giving 80% and 55% probability of getting accepted, which one is good model?\n",
    "\n",
    "The best model is the model which gives higher probabilities to the events happen to us, whether its acceptance or rejection.\n",
    "\n",
    "This method is called ***Maximum Likelihood***. We pick the model which gives the existing labels the highest probability. Thus by maximizing the probability, we can pick the best possible model.\n",
    "\n",
    "\n",
    "<img src=\"assets/maximum_likelihood1.png\" width=900px>\n",
    "\n",
    "\n",
    "### Goal is to maximize the probability:\n",
    "\n",
    "<img src=\"assets/maximum_likelihood2.png\" width=900px>\n",
    "\n",
    "This method is called maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='maximum_likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Probabilities\n",
    "\n",
    "In this lesson and quiz, we will learn how to maximize a probability, using some math. Nothing more than high school math, so get ready for a trip down memory lane!\n",
    "\n",
    "*Is there any connection between error function and Probability?* Let's see.\n",
    "\n",
    "\n",
    "### Issue with product\n",
    "\n",
    "<img src=\"assets/product_to_sum.png\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='product_to_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "<img src=\"assets/cross_entropy1.PNG\" width=900px>\n",
    "\n",
    "**Low Value -> Good cross entropy**\n",
    "\n",
    "**High Value -> Bad cross entropy**\n",
    "\n",
    "\n",
    "* *Negative of logarithm of large number is small number*\n",
    "\n",
    "#### Value of misclassified points is more\n",
    "\n",
    "<img src=\"assets/cross_entropy2.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a bunch of events and probabilities. How likely is that those events happen based on probabilities?\n",
    "\n",
    "**If it's very likely -> small cross entropy**\n",
    "\n",
    "**If it's unlikely -> large cross entropy**\n",
    "\n",
    "\n",
    "#### Let's elaborate\n",
    "\n",
    "<img src=\"assets/cross_entropy3.png\" width=900px>\n",
    "\n",
    "<img src=\"assets/cross_entropy4.png\" width=900px>\n",
    "\n",
    "<img src=\"assets/cross_entropy5.png\" width=900px>\n",
    "\n",
    "<img src=\"assets/cross_entropy6.png\" width=900px>\n",
    "\n",
    "***The formula really encompasses the sums of negatives of logarithms which is precisely the cross entropy.***\n",
    "\n",
    "***So, the cross entropy really tells us, when two vectors are similar or different.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Cross-Entropy\n",
    "\n",
    "<img src=\"assets/multi_class_cross_entropy1.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/multi_class_cross_entropy2.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/multi_class_cross_entropy3.PNG\" width=900px>\n",
    "\n",
    "Let's add some parameters:\n",
    "\n",
    "<img src=\"assets/multi_class_cross_entropy4.PNG\" width=900px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_quiz(name='cross_entropy_probability_relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Coding Cross-entropy\n",
    "Now, time to shine! Let's code the formula for cross-entropy in Python.\n",
    "\n",
    "Refer exercise notebook for softmax coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Now, we're finally ready for one of the most popular and useful algorithms in Machine Learning, and the building block of all that constitutes Deep Learning. The Logistic Regression Algorithm. And it basically goes like this:\n",
    "\n",
    "* Take your data\n",
    "* Pick a random model\n",
    "* Calculate the error\n",
    "* Minimize the error, and obtain a better model\n",
    "* Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Error Function\n",
    "\n",
    "<img src=\"assets/cross_entropy7.PNG\" width=900px>\n",
    "\n",
    "We concluded that 2nd model is better, because the cross entropy is much smaller.\n",
    "\n",
    "<img src=\"assets/error_function1.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/error_function2.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/error_function3.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/error_function4.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/error_function5.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/error_function6.PNG\" width=900px>\n",
    "\n",
    "***Given that we have formula for two classes and m classes, these formulae look different. But it's a nice exercise to convince yourself that the two are the same.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the error function\n",
    "\n",
    "<img src=\"assets/error_function7.PNG\" width=900px>\n",
    "\n",
    "***We will use Gradient Descent to minimize the error function.***\n",
    "\n",
    "<img src=\"assets/error_function8.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "In this lesson, we'll learn the principles and the math behind the gradient descent algorithm.\n",
    "\n",
    "<img src=\"assets/gradient_descent1.PNG\" width=900px>\n",
    "\n",
    "Gradient is given by vector some of partial derivatives of $w_1$ w.r.t E and $w_2$ w.r.t E. This gradient tells us the direction we should move, if we want to increase the error function the most. Thus -ve of the gradient will tell us how to decrease the error function the most.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/gradient_descent2.PNG\" width=900px>\n",
    "\n",
    "Once we take a step, we will be in a lower position. So, we'll do it again and again, until we are able to get to the bottom of mountain.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/gradient_descent3.PNG\" width=900px>\n",
    "\n",
    "This is how we calculate the gradient. As before, we don't want to make any dramatic changes, so we introduce the learning rate $\\alpha$ and we will multiply the gradient by that number.\n",
    "\n",
    "By updating the weights and bias, we can conclude that the prediction we have now is better than previous prediction. This is precisely the ***Gradient Descent*** step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Calculation\n",
    "In the last few lessons, we learned that in order to minimize the error function, we need to take some derivatives. So let's get our hands dirty and actually compute the derivative of the error function. The first thing to notice is that the sigmoid function has a really nice derivative. Namely,\n",
    "\n",
    "$$\\sigma^{\\prime}(x) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "\n",
    "Now, let's do some mathematics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='scalar_gradient_relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "If this sounds anything like the perceptron algorithm, this is no coincidence! We'll see it in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Step\n",
    "\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights and bias in the shown way.\n",
    "\n",
    "*Note:* Since we've taken the average of the errors, the term we are adding should be $\\frac{1}{m} \\cdot \\alpha$ instead of $\\alpha$, but as $\\alpha$ is a constant, then in order to simplify calculations, we'll just take $\\frac{1}{m} \\cdot \\alpha$  to be our learning rate, and abuse the notation by just calling it $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Algorithm\n",
    "\n",
    "<img src=\"assets/logistic_regression1.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/logistic_regression2.PNG\" width=900px>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/logistic_regression3.PNG\" width=900px>\n",
    "\n",
    "Have we seen something like that before?\n",
    "\n",
    "We look at each point and what each point is doing is adding a multiple of itself into the weights of the line, in order to  get the line to move closer towards it if it's misclassified.\n",
    "This is pretty much what perceptron algorithm was doing. We'll look at the similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Implementing Gradient Descent\n",
    "In the following notebook, you'll be able to implement the gradient descent algorithm on the following sample dataset with two classes.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"assets/dataset.png\" width=450px>\n",
    "    Red and blue data points with some overlap.\n",
    "</div>\n",
    "\n",
    "### Workspace\n",
    "To open this notebook, you have following option:\n",
    "\n",
    "> * Clone the repo from [Github](https://github.com/ashwinwadte/deep-learning-v2-pytorch) and open the notebook ***GradientDescent.ipynb*** in the ***intro-neural-networks > gradient-descent*** folder. You can either download the repository via the command line with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/ashwinwadte/deep-learning-v2-pytorch/archive/master.zip).\n",
    "\n",
    "### Instructions\n",
    "In this notebook, you'll be implementing the functions that build the gradient descent algorithm, namely:\n",
    "\n",
    "`sigmoid`: The sigmoid activation function.\n",
    "\n",
    "`output_formula`: The formula for the prediction.\n",
    "\n",
    "`error_formula`: The formula for the error at a point.\n",
    "\n",
    "`update_weights`: The function that updates the parameters with one gradient descent step.\n",
    "\n",
    "When you implement them, run the `train` function and this will graph the several of the lines that are drawn in successive gradient descent steps. It will also graph the error function, and you can see it decreasing as the number of epochs grows.\n",
    "\n",
    "This is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "<img src=\"assets/gd_vs_perceptron1.PNG\" width=900px>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/gd_vs_perceptron2.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Perceptron\n",
    "\n",
    "<img src=\"assets/continuous_perceptron1.PNG\" width=900px>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/continuous_perceptron2.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Models\n",
    "\n",
    "<img src=\"assets/non_linear_models1.PNG\" width=900px>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/non_linear_models2.PNG\" width=900px>\n",
    "\n",
    "***Everything will be the same as before except this boundary equation will be not be linear. That's where neural network comes into play.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (NN) Architecture\n",
    "Ok, so we're ready to put these building blocks together, and build great Neural Networks! (Or Multi-Layer Perceptrons, however you prefer to call them.)\n",
    "\n",
    "<img src=\"assets/nn_arch1.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch2.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch3.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch4.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch5.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch6.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch7.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch8.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch9.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch10.PNG\" width=900px>\n",
    "<img src=\"assets/nn_arch11.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name=\"perceprton_weights_bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Multiple layers\n",
    "Now, not all neural networks look like the one above. They can be way more complicated! In particular, we can do the following things:\n",
    "\n",
    "* Add more nodes to the input, hidden, and output layers.\n",
    "* Add more layers.\n",
    "\n",
    "We'll see the effects of these changes.\n",
    "\n",
    "#### Architecture of neural network\n",
    "\n",
    "<img src=\"assets/nn_arch12.PNG\" width=900px>\n",
    "\n",
    "###### More hidden nodes\n",
    "<img src=\"assets/nn_arch13.PNG\" width=900px>\n",
    "\n",
    "###### More input nodes\n",
    "<img src=\"assets/nn_arch14.PNG\" width=900px>\n",
    "\n",
    "###### More output nodes\n",
    "<img src=\"assets/nn_arch15.PNG\" width=900px>\n",
    "\n",
    "###### More layers - deep neural network\n",
    "<img src=\"assets/nn_arch16.PNG\" width=900px>\n",
    "\n",
    "##### In general we can do this many times and obtain highly complex model with lots of hidden layers\n",
    "<img src=\"assets/nn_arch17.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification\n",
    "And here we elaborate a bit more into what can be done if our neural network needs to model data with more than one output.\n",
    "\n",
    "<img src=\"assets/nn_arch18.PNG\" width=900px>\n",
    "But it seems like ovekill.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"assets/nn_arch19.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "Feedforward is the process, neural networks use to turn the input into an output. Let's study it more carefully, before we dive into how to train the networks.\n",
    "\n",
    "<img src=\"assets/feed_forward1.PNG\" width=900px>\n",
    "<img src=\"assets/feed_forward2.PNG\" width=900px>\n",
    "<img src=\"assets/feed_forward3.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function\n",
    "Just as before, neural networks will produce an error function, which at the end, is what we'll be minimizing. Let's see the error function for a neural network.\n",
    "\n",
    "<img src=\"assets/feed_forward4.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Now, we're ready to get our hands into training a neural network. For this, we'll use the method known as backpropagation. In a nutshell, backpropagation will consist of:\n",
    "\n",
    "* Doing a feedforward operation.\n",
    "* Comparing the output of the model with the desired output.\n",
    "* Calculating the error.\n",
    "* Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "* Use this to update the weights, and get a better model.\n",
    "* Continue this until we have a model that is good.\n",
    "\n",
    "\n",
    "Sounds more complicated than what it actually is. Let's take a look. First we will see a conceptual interpretation of what backpropagation is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Multi - layer perceptron\n",
    "For single perceptron, we saw the gradient descent, we will apply same logic for multi - layer perceptron.\n",
    "\n",
    "<img src=\"assets/back_propagation1.PNG\" width=900px>\n",
    "\n",
    "**NOTE**: We are only considering weights for simplicity, but in reality, we will update the bias as well.\n",
    "\n",
    "\n",
    "<img src=\"assets/back_propagation2.PNG\" width=900px>\n",
    "<img src=\"assets/back_propagation3.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "<img src=\"assets/back_propagation4.PNG\" width=900px>\n",
    "\n",
    "**Note** : Feedforwading is literally composing the functions and back propagation is literally taking the derivative at each piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation in details\n",
    "<img src=\"assets/back_propagation5.PNG\" width=900px>\n",
    "<img src=\"assets/back_propagation6.PNG\" width=900px>\n",
    "<img src=\"assets/back_propagation7.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyzing Student Data\n",
    "Now, we're ready to put neural networks in practice. We'll analyze a dataset of student admissions at UCLA.\n",
    "\n",
    "To open this notebook, you have following option:\n",
    "\n",
    "> * Clone the repo from [Github](https://github.com/ashwinwadte/deep-learning-v2-pytorch) and open the notebook ***StudentAdmissions.ipynb*** in the ***intro-neural-networks > student_admissions*** folder. You can either download the repository with `git clone https://github.com/udacity/deep-learning-v2-pytorch.git`, or download it as an archive file from [this link](https://github.com/ashwinwadte/deep-learning-v2-pytorch/archive/master.zip).\n",
    "\n",
    "### Instructions\n",
    "In this notebook, you'll be implementing some of the steps in the training of the neural network, namely:\n",
    "\n",
    "* One-hot encoding the data\n",
    "* Scaling the data\n",
    "* Writing the backpropagation step\n",
    "\n",
    "\n",
    "This is a self-assessed lab. If you need any help or want to check your answers, feel free to check out the solutions notebook in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now we know how to build and train neural network to fit our data, but sometimes we try to train and see that nothing works as planned. Why? Because there are many things which can fail, like our architecture may be poorly chosen, data can be noisy, our model may be taking years to run. We need to find ways to optimize the training. Let's see, how we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "<img src=\"assets/testing1.PNG\" width=900px>\n",
    "<img src=\"assets/testing2.PNG\" width=900px>\n",
    "<img src=\"assets/testing3.PNG\" width=900px>\n",
    "<img src=\"assets/testing4.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "So let's talk about life! In life there are two mistakes one can make. One may try to kill godzilla with fly swatter i.e. oversimplifing the problem (in ML, it is called **Underfitting**) or fly with bazuka i.e. over complicated when we can use much simpler solution instead (in ML, it is called **Overfitting**).\n",
    "\n",
    "<img src=\"assets/fitting1.PNG\" width=900px>\n",
    "<img src=\"assets/fitting2.PNG\" width=900px>\n",
    "\n",
    "---\n",
    "<img src=\"assets/fitting3.PNG\" width=900px>\n",
    "\n",
    "Sometimes we refer **underfitting** as **Error due to bias**.\n",
    " \n",
    "---\n",
    "<img src=\"assets/fitting4.PNG\" width=900px>\n",
    "<img src=\"assets/fitting5.PNG\" width=900px>\n",
    "<img src=\"assets/fitting6.PNG\" width=900px>\n",
    "\n",
    "Sometimes we refer **overfitting** as **Error due to variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/fitting7.PNG\" width=900px>\n",
    "<img src=\"assets/fitting8.PNG\" width=900px>\n",
    "\n",
    "In real life, we will always end up in underfitting or overfitting. Should we go for smaller pant or bigger pant? It's less bad to go for bigger pant and try to get a belt?\n",
    "\n",
    "That's what we are going to do, we will err on the side of overly complicated models and then we'll apply certain techniques to prevent overfitting on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "So let's start with a complicated network architecture which would be more complicated than we need but we need to live with it.\n",
    "\n",
    "<img src=\"assets/fitting9.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/fitting10.PNG\" width=900px>\n",
    "<img src=\"assets/fitting11.PNG\" width=900px>\n",
    "\n",
    "\n",
    "So, we do gradient descent until the testing error stops decreasing and starts to increase. At that moment, we stop. This algorithm is called **Early stopping** and is widely used to train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "<img src=\"assets/regularization1.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_quiz(name='regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"assets/regularization2.PNG\" width=900px>\n",
    "So, there is overfitting but in subtle way.\n",
    "\n",
    "<img src=\"assets/regularization3.PNG\" width=900px>\n",
    "It is much harder to do gradient descent in 2nd case, since the derivatives are mostly close to zero and then very large when we get to the middle of the curve. Therefore, in order to do gradient descent properly, we want the model in the left more than the one in the right.\n",
    "\n",
    "This is very well summarized by:\n",
    "<img src=\"assets/regularization4.PNG\" width=900px>\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "<img src=\"assets/regularization5.PNG\" width=900px>\n",
    "\n",
    "<img src=\"assets/regularization6.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "<img src=\"assets/dropout1.PNG\" width=900px>\n",
    "<img src=\"assets/dropout2.PNG\" width=900px>\n",
    "<img src=\"assets/dropout3.PNG\" width=900px>\n",
    "<img src=\"assets/dropout4.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Minima\n",
    "\n",
    "<img src=\"assets/local_minima1.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random restart\n",
    "\n",
    "<img src=\"assets/local_minima2.PNG\" width=900px>\n",
    "<img src=\"assets/local_minima3.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient\n",
    "\n",
    "<img src=\"assets/vanishing_gradient1.PNG\" width=900px>\n",
    "<img src=\"assets/vanishing_gradient2.PNG\" width=900px>\n",
    "<img src=\"assets/vanishing_gradient3.PNG\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Activation Functions\n",
    "\n",
    "The best way to fix this vanishing gradient issue is to change the activation functions.\n",
    "\n",
    "<img src=\"assets/activation_functions1.PNG\" width=900px>\n",
    "<img src=\"assets/activation_functions2.PNG\" width=900px>\n",
    "<img src=\"assets/activation_functions3.PNG\" width=900px>\n",
    "<img src=\"assets/activation_functions4.PNG\" width=900px>\n",
    "\n",
    "Note that the last unit is a sigmoid, since our final output still needs to be a probability between 0 and 1. However, if we let the final unit be a ReLU, we can actually end up with regression models that predict a value. This will be of use in the recurrent neural network section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch vs Stochastic Gradient Descent\n",
    "\n",
    "<img src=\"assets/bsgd1.PNG\" width=900px>\n",
    "<img src=\"assets/bsgd2.PNG\" width=900px>\n",
    "<img src=\"assets/bsgd3.PNG\" width=900px>\n",
    "<img src=\"assets/bsgd4.PNG\" width=900px>\n",
    "<img src=\"assets/bsgd5.PNG\" width=900px>\n",
    "<img src=\"assets/bsgd6.PNG\" width=900px>\n",
    "\n",
    "Notice that with the data, we took four steps whereas, when we did normal gradient descent, we took only one step with all the data. Of course, the four steps we took were less accurate but in practice, it's much better to take a bunch of slightly inaccurate steps than to take one good one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Decay\n",
    "\n",
    "<img src=\"assets/momentum1.PNG\" width=900px>\n",
    "<img src=\"assets/momentum2.PNG\" width=900px>\n",
    "\n",
    "# Momentum\n",
    "<img src=\"assets/momentum3.PNG\" width=900px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
